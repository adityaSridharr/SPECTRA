{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai-clip\n",
      "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ftfy\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /home/aditya_sridhar/.local/lib/python3.10/site-packages (from openai-clip) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /home/aditya_sridhar/.local/lib/python3.10/site-packages (from openai-clip) (4.66.4)\n",
      "Requirement already satisfied: wcwidth in /home/aditya_sridhar/.local/lib/python3.10/site-packages (from ftfy->openai-clip) (0.2.13)\n",
      "Building wheels for collected packages: openai-clip\n",
      "  Building wheel for openai-clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368647 sha256=da59b05af418fc1585ea5028fd279bd8c91f534ea201715a32c7aab6f1bb5161\n",
      "  Stored in directory: /home/aditya_sridhar/.cache/pip/wheels/08/77/8e/8d2f862df6bf7fb4e2007062d2cbaeae49862ec7b56d041229\n",
      "Successfully built openai-clip\n",
      "Installing collected packages: ftfy, openai-clip\n",
      "Successfully installed ftfy-6.3.1 openai-clip-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import clip\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Path to the CUB dataset root directory\n",
    "CUB_ROOT = './data/CUB_200_2011/CUB_200_2011/'\n",
    "\n",
    "# CSV File Setup for Logging Results\n",
    "RESULTS_CSV_PATH = 'hyperparameter_tuning_complex_cnn_results.csv'\n",
    "\n",
    "# Ensure the results CSV file has headers\n",
    "if not os.path.exists(RESULTS_CSV_PATH):\n",
    "    with open(RESULTS_CSV_PATH, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            \"Model_Architecture_Index\", \"Learning_Rate\", \n",
    "            \"Classification_Weight\", \"Attribute_Weight\", \n",
    "            \"Training_Loss\", \"Training_Accuracy\", \n",
    "            \"Attribute_Training_Loss\", \"Classification_Test_Accuracy\", \n",
    "            \"Test_Loss\"\n",
    "        ])\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LABEL_VECTOR_SIZE = 15  # Labels to include in training/testing\n",
    "IMAGE_SIZE = (150, 150)  # Resizing dimensions for images\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Attributes: 100%|██████████| 11788/11788 [00:53<00:00, 220.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning with config: {'model_index': 4, 'learning_rate': 0.001, 'classification_weight': 0.3, 'attribute_weight': 0.7}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 150, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (17) must match the size of tensor b (50) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 222\u001b[0m\n\u001b[1;32m    218\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    220\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 222\u001b[0m \u001b[43mtrain_and_evaluate_clip_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 177\u001b[0m, in \u001b[0;36mtrain_and_evaluate_clip_model\u001b[0;34m(train_loader, test_loader, device, num_epochs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 177\u001b[0m     train_loss, train_class_loss, train_attr_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclassification_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassification_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribute_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattribute_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     eval_loss, eval_class_loss, eval_attr_loss \u001b[38;5;241m=\u001b[39m evaluate_epoch(\n\u001b[1;32m    183\u001b[0m         model, test_loader, device,\n\u001b[1;32m    184\u001b[0m         classification_weight\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    185\u001b[0m         attribute_weight\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    186\u001b[0m     )\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Eval Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 105\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, device, classification_weight, attribute_weight)\u001b[0m\n\u001b[1;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 105\u001b[0m predicted_attributes, class_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m loss, class_loss, attr_loss \u001b[38;5;241m=\u001b[39m calculate_losses(predicted_attributes, class_logits, attributes, labels, classification_weight, attribute_weight)\n\u001b[1;32m    107\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 73\u001b[0m, in \u001b[0;36mCLIPWithComplexCNNClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Extract features using the CLIP model\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# CLIP's image encoder\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Pass through the attribute classifier\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_attributes(image_features)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/clip/model.py:342\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/clip/model.py:229\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    227\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, grid ** 2, width]\u001b[39;00m\n\u001b[1;32m    228\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_embedding\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice), x], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, grid ** 2 + 1, width]\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_pre(x)\n\u001b[1;32m    232\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (17) must match the size of tensor b (50) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dataset Classes\n",
    "class CUBAttributeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading CUB-200-2011 images, attributes, and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load metadata\n",
    "        self.images_df = pd.read_csv(os.path.join(root_dir, 'images.txt'), sep=' ', header=None, names=['image_id', 'image_path'])\n",
    "        self.labels_df = pd.read_csv(os.path.join(root_dir, 'image_class_labels.txt'), sep=' ', header=None, names=['image_id', 'class_id'])\n",
    "        attributes_path = os.path.join(root_dir, 'attributes/image_attribute_labels.txt')\n",
    "        self.attributes_df = self._load_attributes(attributes_path)\n",
    "\n",
    "        # Process attributes into vectors\n",
    "        self.attribute_vectors = self._process_attributes()\n",
    "\n",
    "    def _load_attributes(self, filepath):\n",
    "        # Read the file and filter rows with exactly 5 columns\n",
    "        attributes_data = []\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                row = line.split()\n",
    "                if len(row) == 5:\n",
    "                    attributes_data.append(row)\n",
    "\n",
    "        return pd.DataFrame(attributes_data, columns=['image_id', 'attribute_id', 'is_present', 'certainty', 'time']).astype({\n",
    "            'image_id': int, 'attribute_id': int, 'is_present': int, 'certainty': float, 'time': float\n",
    "        })\n",
    "\n",
    "    def _process_attributes(self):\n",
    "        # Create a dictionary mapping image IDs to attribute vectors\n",
    "        attribute_vectors = {}\n",
    "        for image_id in tqdm(self.images_df['image_id'], desc=\"Processing Attributes\"):\n",
    "            attributes = self.attributes_df[self.attributes_df['image_id'] == image_id]\n",
    "            attribute_vector = torch.zeros(312)  # 312 attributes\n",
    "            attribute_ids = attributes[attributes['is_present'] == 1]['attribute_id']\n",
    "            attribute_vector[attribute_ids.values - 1] = 1\n",
    "            attribute_vectors[image_id] = attribute_vector\n",
    "        return attribute_vectors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_path = os.path.join(self.root_dir, 'images', self.images_df.iloc[idx, 1])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get attribute vector and label\n",
    "        image_id = self.images_df.iloc[idx, 0]\n",
    "        attribute_vector = self.attribute_vectors[image_id]\n",
    "        label = self.labels_df[self.labels_df['image_id'] == image_id]['class_id'].values[0] - 1  # Zero-indexed\n",
    "\n",
    "        return image, attribute_vector, label\n",
    "\n",
    "\n",
    "# Models\n",
    "class CLIPWithComplexCNNClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, attribute_vector_size=312, num_classes=200):\n",
    "        super(CLIPWithComplexCNNClassifier, self).__init__()\n",
    "        \n",
    "        self.clip_model = clip_model\n",
    "        self.fc_attributes = nn.Linear(512, attribute_vector_size)  # Assuming the output features from CLIP are 512-dimensional\n",
    "        self.fc_classifier = nn.Linear(attribute_vector_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the CLIP model\n",
    "        image_features = self.clip_model.encode_image(x)  # CLIP's image encoder\n",
    "        # Pass through the attribute classifier\n",
    "        attributes = self.fc_attributes(image_features)\n",
    "        # Pass through the final classification layer\n",
    "        class_logits = self.fc_classifier(attributes)\n",
    "        return attributes, class_logits\n",
    "\n",
    "\n",
    "# Loss Function\n",
    "def calculate_losses(attribute_vector, class_logits, true_attribute, true_label, classification_weight=0.5, attribute_weight=0.5):\n",
    "    \"\"\"\n",
    "    Calculate combined losses for attributes and classification.\n",
    "    \"\"\"\n",
    "    attribute_loss = F.binary_cross_entropy_with_logits(attribute_vector, true_attribute)\n",
    "    classification_loss = F.cross_entropy(class_logits, true_label)\n",
    "    total_loss = classification_weight * classification_loss + attribute_weight * attribute_loss\n",
    "    return total_loss, classification_loss, attribute_loss\n",
    "\n",
    "\n",
    "# Training and Evaluation Functions\n",
    "def train_epoch(model, train_loader, optimizer, device, classification_weight=0.5, attribute_weight=0.5):\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = total_class_loss = total_attribute_loss = 0\n",
    "\n",
    "    for images, attributes, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        images, attributes, labels = images.to(device), attributes.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        print(images.shape)\n",
    "        predicted_attributes, class_logits = model(images)\n",
    "        loss, class_loss, attr_loss = calculate_losses(predicted_attributes, class_logits, attributes, labels, classification_weight, attribute_weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_class_loss += class_loss.item()\n",
    "        total_attribute_loss += attr_loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader), total_class_loss / len(train_loader), total_attribute_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate_epoch(model, test_loader, device, classification_weight=0.5, attribute_weight=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = total_class_loss = total_attribute_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, attributes, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, attributes, labels = images.to(device), attributes.to(device), labels.to(device)\n",
    "            predicted_attributes, class_logits = model(images)\n",
    "            loss, class_loss, attr_loss = calculate_losses(predicted_attributes, class_logits, attributes, labels, classification_weight, attribute_weight)\n",
    "            total_loss += loss.item()\n",
    "            total_class_loss += class_loss.item()\n",
    "            total_attribute_loss += attr_loss.item()\n",
    "\n",
    "    return total_loss / len(test_loader), total_class_loss / len(test_loader), total_attribute_loss / len(test_loader)\n",
    "\n",
    "def evaluate_accuracy(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the classification accuracy on the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            _, class_logits = model(images)\n",
    "            predicted_labels = class_logits.argmax(dim=1)\n",
    "            correct += (predicted_labels == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def train_and_evaluate_clip_model(train_loader, test_loader, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Hyperparameter tuning for the CLIP model with a Complex CNN classifier.\n",
    "    \"\"\"\n",
    "    hyperparameter_grid = [\n",
    "        {\"model_index\": 4, \"learning_rate\": 1e-3, \"classification_weight\": 0.3, \"attribute_weight\": 0.7},\n",
    "        {\"model_index\": 4, \"learning_rate\": 1e-3, \"classification_weight\": 0.5, \"attribute_weight\": 0.5},\n",
    "        {\"model_index\": 4, \"learning_rate\": 1e-3, \"classification_weight\": 0.7, \"attribute_weight\": 0.3},\n",
    "        {\"model_index\": 4, \"learning_rate\": 5e-4, \"classification_weight\": 0.3, \"attribute_weight\": 0.7},\n",
    "        {\"model_index\": 4, \"learning_rate\": 5e-4, \"classification_weight\": 0.5, \"attribute_weight\": 0.5},\n",
    "        {\"model_index\": 4, \"learning_rate\": 5e-4, \"classification_weight\": 0.7, \"attribute_weight\": 0.3},\n",
    "    ]\n",
    "\n",
    "    # Load pretrained CLIP model\n",
    "    clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "    for config in hyperparameter_grid:\n",
    "        print(f\"Tuning with config: {config}\")\n",
    "        model = CLIPWithComplexCNNClassifier(clip_model, attribute_vector_size=312, num_classes=200)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "        model.to(device)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            train_loss, train_class_loss, train_attr_loss = train_epoch(\n",
    "                model, train_loader, optimizer, device,\n",
    "                classification_weight=config[\"classification_weight\"],\n",
    "                attribute_weight=config[\"attribute_weight\"]\n",
    "            )\n",
    "            eval_loss, eval_class_loss, eval_attr_loss = evaluate_epoch(\n",
    "                model, test_loader, device,\n",
    "                classification_weight=config[\"classification_weight\"],\n",
    "                attribute_weight=config[\"attribute_weight\"]\n",
    "            )\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        classification_test_accuracy = evaluate_accuracy(model, test_loader, device)\n",
    "\n",
    "        # Log results to CSV\n",
    "        with open(RESULTS_CSV_PATH, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\n",
    "                config[\"model_index\"], config[\"learning_rate\"], \n",
    "                config[\"classification_weight\"], config[\"attribute_weight\"], \n",
    "                train_loss, classification_test_accuracy, train_attr_loss, \n",
    "                eval_loss, eval_class_loss\n",
    "            ])\n",
    "\n",
    "\n",
    "\n",
    "    # Load dataset\n",
    "cub_dataset = CUBAttributeDataset(root_dir=CUB_ROOT, transform=transform)\n",
    "\n",
    "# Filter dataset by labels4\n",
    "INCLUDE_LABELS = list(range(LABEL_VECTOR_SIZE))\n",
    "filtered_indices = [idx for idx, (_, _, label) in enumerate(cub_dataset) if label in INCLUDE_LABELS]\n",
    "filtered_dataset = torch.utils.data.Subset(cub_dataset, filtered_indices)\n",
    "\n",
    "# Split into train/test datasets\n",
    "train_size = int(0.8 * len(filtered_dataset))\n",
    "test_size = len(filtered_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(filtered_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_and_evaluate_clip_model(train_loader, test_loader, device, num_epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting patsy>=0.5.6\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.9/232.9 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<3,>=1.22.3 in /home/aditya_sridhar/.local/lib/python3.10/site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /home/aditya_sridhar/.local/lib/python3.10/site-packages (from statsmodels) (1.13.1)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /home/aditya_sridhar/.local/lib/python3.10/site-packages (from statsmodels) (2.2.3)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/aditya_sridhar/.local/lib/python3.10/site-packages (from statsmodels) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aditya_sridhar/.local/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aditya_sridhar/.local/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aditya_sridhar/.local/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.16.0)\n",
      "Installing collected packages: patsy, statsmodels\n",
      "Successfully installed patsy-1.0.1 statsmodels-0.14.4\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
